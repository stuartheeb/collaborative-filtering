# -*- coding: utf-8 -*-
"""GP_entryFilling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_9MagnLgEbA-PGtjy_CfFZypk0WQam68
"""

import numpy as np
import pandas as pd

"""
Read data and transfer to numpy
"""

x_train_data = pd.read_csv('drive/MyDrive/CIL Project/data_train.csv').to_numpy()

print("X_train shape: ", x_train_data.shape)

def extract_users_items_predictions(data_pd):
    users, movies = \
        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\d+)_c(\d+)').values.astype(int) - 1, 2, axis=-1)]
    predictions = data_pd.Prediction.values
    return users, movies, predictions


def Create_Matrix():
    data_pd = pd.read_csv('drive/MyDrive/CIL Project/data_train.csv')
    users, movies, predictions = extract_users_items_predictions(data_pd)
    number_of_users = np.amax(users)+1
    number_of_movies = np.amax(movies)+1
    data = np.full((number_of_users, number_of_movies), np.nan)
    for user, movie, pred in zip(users, movies, predictions):
       data[user][movie] = pred

    
    total_ratings = 0.0
    mask_data = np.zeros((number_of_users, number_of_movies))
    for i in range(number_of_users):
        for j in range(number_of_movies):
            if not np.isnan(data[i][j]):
                total_ratings += 1.0
                mask_data[i][j] = 1
    

    rating_percentage = total_ratings / (number_of_users * number_of_movies)
    print('There are a total of ' + str(total_ratings) + ' ratings, which means that ' + str(rating_percentage) + ' of all entries exist')
    
    return data.astype(float),mask_data.astype(int),users,movies,predictions

x_train_pd = pd.read_csv('drive/MyDrive/CIL Project/data_train.csv')
users,movies,predictions = extract_users_items_predictions(x_train_pd)
print(users)
print(movies)
print(predictions)

data,mask_data,users,movies,predictions = Create_Matrix()

print(data)

def KAA_splicer(covmat, selection_A):
  assert(selection_A.shape[0] == covmat.shape[0])
  assert(selection_A.shape[0] == covmat.shape[1])
  temp = covmat[:, selection_A]
  temp = temp[selection_A, :]
  return temp

def KXA_splicer(covmat, selection_A):
  assert(selection_A.shape[0] == covmat.shape[0])
  assert(selection_A.shape[0] == covmat.shape[1])
  temp = covmat[:, selection_A]
  temp = temp[np.logical_not(selection_A), :]
  return temp

def MUA_splicer(mu, selection_A):
  assert(selection_A.shape[0] == mu.shape[0])
  temp = mu[np.logical_not(selection_A)]
  return temp

def gauss_kernel(a, b, bandwidth):
  return np.exp(-(np.linalg.norm(a-b)**2)/bandwidth**2)

#X is the recommender system Matrix and entry_mask masks the existend entries
def gp_imputation(X, bandwidth, noise):
  #Xcombined = np.concatenate((X, Xtest))
  Xcombined = X
  maskedXcombined = np.ma.array(Xcombined, mask = np.logical_not(np.isnan(Xcombined)))
  mean = np.nanmean(Xcombined, axis = 0)
  cov = np.zeros((Xcombined.shape[1], Xcombined.shape[1]))
  notnan_mask = np.logical_not(np.isnan(Xcombined))
  for i in range(Xcombined.shape[1]):
    localmask = notnan_mask[:, i]
    #cov[i, i] = (np.matmul(Xcombined[localmask, i], Xcombined[localmask, i])**2+1)**2
    cov[i, i] = np.var(Xcombined[localmask, i])
    #cov[i, i] = 1.0
    for j in range(i + 1, Xcombined.shape[1]):
      localmask = notnan_mask[:, i] & notnan_mask[:, j]
      t1 = Xcombined[localmask, i]
      t2 = Xcombined[localmask, j]
      #temp = np.cov([t1, t2])
      #cov[i, j] = (np.matmul(t1, t2) + 1)**2
      #cov[i, j] = temp[0, 1]
      cov[i, j] = gauss_kernel(t1, t2, bandwidth)
      cov[j, i] = cov[i, j]
  Xcombined_new = np.copy(Xcombined)

  print(Xcombined.shape)

  for index, features in enumerate(Xcombined):
    selection_A = notnan_mask[index, :]
    mu = MUA_splicer(mean, selection_A)
    kxa = KXA_splicer(cov, selection_A)
    kaa = KAA_splicer(cov, selection_A)
    ya = Xcombined[index, selection_A]
    #print(mu.shape)
    #print(kxa.shape)
    #print(kaa.shape)
    #print(ya.T.shape)
    pred = mu + np.matmul(kxa, np.linalg.solve(kaa + np.identity(kaa.shape[0]) * noise**2, ya.T))
    #print(pred)
    #assert(False)
    for i in range(Xcombined.shape[1]):
      curr = 0
      if np.logical_not(notnan_mask[index, i]):
        Xcombined_new[index, i] = pred[curr]
        curr += 1
  #assert(not np.any(np.isnan(Xcombined_new)))
  return Xcombined_new

gp_pred = gp_imputation(data,1,1.0)

print(gp_pred)

x_test_pd = pd.read_csv('drive/MyDrive/CIL Project/sampleSubmission.csv')
pred_users,pred_movies,pred_predictions = extract_users_items_predictions(x_test_pd)

#it seems like that the mask is somehow wrong
#check if there are already entries in data 

output = x_test_pd.to_numpy()
for id,user in enumerate(pred_users):
  prediction = gp_pred[user][pred_movies[id]]
  output[id][1] = prediction 
  #print(prediction)
  #print(data[user][pred_movies[id]])

print(output)

submission_df = pd.DataFrame(output, columns = ['Id', 'Prediction'])
print(submission_df)
submission_df.to_csv('VAE.csv',index = False)

