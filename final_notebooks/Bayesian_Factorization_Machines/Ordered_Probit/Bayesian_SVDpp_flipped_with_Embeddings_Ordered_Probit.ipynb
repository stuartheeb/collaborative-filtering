{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfac107a",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ac45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages to apply SVD and Bayesian Factorization Machine\n",
    "!pip install surprise\n",
    "\n",
    "!pip install myfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db612bf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402f6563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the data\n",
    "import pandas as pd\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To apply SVD\n",
    "import surprise\n",
    "\n",
    "# To apply Factorization Machines\n",
    "import myfm\n",
    "from myfm import RelationBlock\n",
    "from myfm.utils.encoders import(DataFrameEncoder,MultipleValuesToSparseEncoder,CategoryValueToSparseEncoder)\n",
    "\n",
    "# To do train-test split for evaluation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0990a58",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec85b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "\n",
    "!mkdir ~/.kaggle\n",
    "\n",
    "import json\n",
    "\n",
    "kaggle_username = \"yuvalnis\" #@param {type:\"string\"}\n",
    "kaggle_api_key = \"1800d5a286834f0416c338c7bd7f6dee\" #@param {type:\"string\"}\n",
    "\n",
    "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
    "\n",
    "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
    "\n",
    "with open('kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!mv kaggle.json ~/.kaggle/kaggle.json\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c cil-collaborative-filtering-2022\n",
    "\n",
    "!unzip -n cil-collaborative-filtering-2022.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b0f921",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f8286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Extract users, movies and ratings from raw data\n",
    "def extract_users_items_predictions(data_pd):\n",
    "    users, movies = \\\n",
    "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
    "    predictions = data_pd.Prediction.values\n",
    "    return users, movies, predictions\n",
    "\n",
    "#Load raw data\n",
    "data_pd = pd.read_csv('data_train.csv')\n",
    "\n",
    "#Do Train-Test Split\n",
    "train, val = train_test_split(data_pd, test_size=0.1,random_state=42)\n",
    "\n",
    "#Extract users, movies and ratings from raw data\n",
    "total_users, total_movies, total_pred = extract_users_items_predictions(data_pd)\n",
    "train_users, train_movies, train_pred = extract_users_items_predictions(train)\n",
    "val_users, val_movies, val_pred = extract_users_items_predictions(val)\n",
    "\n",
    "#Store total data\n",
    "ratings_dict_total = {'userID': total_users,'movieID': total_movies,'rating': total_pred}\n",
    "df_total = pd.DataFrame(ratings_dict_total)\n",
    "\n",
    "#Store train data\n",
    "ratings_dict_train = {'userID': train_users,'movieID': train_movies,'rating': train_pred}\n",
    "df_train = pd.DataFrame(ratings_dict_train)\n",
    "\n",
    "#Store validation data\n",
    "ratings_dict_test = {'userID': val_users,'movieID': val_movies,'rating': val_pred}\n",
    "df_test = pd.DataFrame(ratings_dict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964582e4",
   "metadata": {},
   "source": [
    "## Apply SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b012a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surprise_svd(df):\n",
    "    \n",
    "    # Transform Data in Desired Format\n",
    "    reader = surprise.Reader(rating_scale=(0,5.5))\n",
    "    surprise_data = surprise.Dataset.load_from_df(df[['userID', 'movieID', 'rating']], reader=reader)\n",
    "    trainset = surprise_data.build_full_trainset()\n",
    "    \n",
    "    #Apply SVD\n",
    "    algo = surprise.SVD(n_factors=3, n_epochs=50,random_state=1234, biased=False)\n",
    "    algo.fit(trainset)\n",
    "    \n",
    "    #Get User/Movie Factors\n",
    "    U = algo.pu\n",
    "    Q = algo.qi\n",
    "    return U,Q\n",
    "\n",
    "U,Q = surprise_svd(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340fb0a",
   "metadata": {},
   "source": [
    "## Get embeddings and implicit user/movie Features\n",
    "- code is based on https://github.com/tohtsky/myFM/blob/main/examples/ml-100k-extended.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c754296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to convert list to str\n",
    "def string_agg(int_list) -> str:\n",
    "    return ','.join([str(y) for y in int_list])\n",
    "\n",
    "#Store user vectors\n",
    "user_vec = []\n",
    "for i in range(10000):\n",
    "    user_vec.append(dict(enumerate(np.array(U[i]).flatten(), 1)))\n",
    "\n",
    "#Store movie vectors\n",
    "movie_vec = []\n",
    "for i in range(1000):\n",
    "    movie_vec.append(dict(enumerate(np.array(Q[i]).flatten(), 1)))\n",
    "    \n",
    "\n",
    "#Store features in a df\n",
    "user_f = pd.DataFrame(np.arange(10000),columns=['userID'])\n",
    "user_f['user_implicit_features'] = df_train.groupby('userID').movieID.agg(string_agg).reindex(user_f.index).fillna('')\n",
    "user_f['user_vec'] = user_vec\n",
    "\n",
    "\n",
    "\n",
    "movie_f = pd.DataFrame(np.arange(1000),columns=['movieID'])\n",
    "movie_f['movie_implicit_features'] = df_train.groupby('movieID').userID.agg(string_agg).reindex(movie_f.index).fillna('')\n",
    "movie_f['movie_vec'] = movie_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164bafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Existing MultipleValuesToSparseEncoder to Store different vector values instead on simple encoding\n",
    "from typing import Iterable\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from typing_extensions import Literal\n",
    "\n",
    "class ValuesToSparseEncoder(CategoryValueToSparseEncoder[str]):\n",
    "    \"\"\"The class to N-hot encode a List of items into a sparse matrix representation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        items: Iterable[str],\n",
    "        min_freq: int = 1,\n",
    "        sep: str = \",\",\n",
    "        normalize: bool = True,\n",
    "        handle_unknown: Literal[\"create\", \"ignore\", \"raise\"] = \"create\",\n",
    "    ):\n",
    "        items_flatten = [\n",
    "            y for x in items for y in set(self.string_agg(list(x.keys())).split(sep)) if y\n",
    "        ]  # ignore empty string.\n",
    "        self.sep = sep\n",
    "        self.normalize = normalize\n",
    "        super().__init__(\n",
    "            items_flatten, min_freq=min_freq, handle_unknown=handle_unknown\n",
    "        )\n",
    "\n",
    "    def to_sparse(self, items: Iterable[str]) -> sps.csr_matrix:\n",
    "        indptr = [0]\n",
    "        indices = []\n",
    "        data = []\n",
    "        n_row = 0\n",
    "        cursor = 0\n",
    "        for row in items:\n",
    "            n_row += 1\n",
    "            items = self.string_agg(list(row.keys())).split(self.sep)\n",
    "            indices_local = [index for index in [self._get_index(v) for v in items if v] if index is not None]\n",
    "            if not indices_local:\n",
    "                indptr.append(cursor)\n",
    "                continue\n",
    "            n = len(indices_local)\n",
    "            indices.extend(indices_local)\n",
    "            \n",
    "            #use vector values\n",
    "            values = list(row.values())\n",
    "            data.extend(values)\n",
    "            cursor += n\n",
    "            indptr.append(cursor)\n",
    "        return sps.csr_matrix(\n",
    "            (data, indices, indptr),\n",
    "            shape=(n_row, len(self)),\n",
    "        )\n",
    "    def string_agg(self,int_list) -> str:\n",
    "        return ','.join([str(y) for y in int_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6613ea",
   "metadata": {},
   "source": [
    "# Transform Data in myFM Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42078c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create User Encoder\n",
    "user_encoder = DataFrameEncoder().add_column(\n",
    "    'userID', CategoryValueToSparseEncoder(user_f.userID)\n",
    "    )\n",
    "\n",
    "user_encoder.add_column(\n",
    "        'user_implicit_features',\n",
    "        MultipleValuesToSparseEncoder(user_f.user_implicit_features, normalize=True)\n",
    "    )\n",
    "\n",
    "user_encoder.add_column(\n",
    "        'user_vec',\n",
    "        ValuesToSparseEncoder(user_f.user_vec)\n",
    "    )\n",
    "\n",
    "#Create Movie Encoder\n",
    "movie_encoder = DataFrameEncoder().add_column(\n",
    "    'movieID', CategoryValueToSparseEncoder(movie_f.movieID)\n",
    "    )\n",
    "movie_encoder.add_column(\n",
    "        'movie_implicit_features',\n",
    "        MultipleValuesToSparseEncoder(movie_f.movie_implicit_features, normalize=True)\n",
    "    )\n",
    "movie_encoder.add_column(\n",
    "        'movie_vec',\n",
    "        ValuesToSparseEncoder(movie_f.movie_vec)\n",
    "    )\n",
    "\n",
    "\n",
    "def augment_user_id(user_ids):\n",
    "    return user_encoder.encode_df(\n",
    "        user_f.reindex(user_ids).reset_index()\n",
    "    )\n",
    "\n",
    "def augment_movie_id(movie_ids):\n",
    "    return movie_encoder.encode_df(\n",
    "        movie_f.reindex(movie_ids).reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "#Create Train/Test Blocks\n",
    "train_blocks = []\n",
    "test_blocks = []\n",
    "for source, target in [(df_train, train_blocks),(df_test, test_blocks)]:\n",
    "    unique_users, user_map = np.unique(source.userID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(user_map, augment_user_id(unique_users))\n",
    "    )\n",
    "    unique_movies, movie_map = np.unique(source.movieID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(movie_map, augment_movie_id(unique_movies))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0775f",
   "metadata": {},
   "source": [
    "## Evaluation and Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a1295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.12, cutpoint = ['-2.252', '-1.509', '-0.542', '0.334'] : 100%|█| 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9716133408375609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.11, cutpoint = ['-2.265', '-1.525', '-0.556', '0.321'] : 100%|█| 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9706007622635014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.15, cutpoint = ['-2.283', '-1.543', '-0.572', '0.305'] : 100%|█| 300/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702002573744777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.14, cutpoint = ['-2.283', '-1.540', '-0.570', '0.304'] : 100%|█| 400/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9699700559012089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.13, cutpoint = ['-2.260', '-1.516', '-0.547', '0.330'] : 100%|█| 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9698330869851077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.14, cutpoint = ['-2.162', '-1.421', '-0.444', '0.435'] : 100%|█| 100/100 \n",
      "w0 = 0.14, cutpoint = ['-2.126', '-1.376', '-0.397', '0.488'] : 100%|█| 200/200 \n",
      "w0 = 0.12, cutpoint = ['-2.124', '-1.375', '-0.397', '0.485'] : 100%|█| 300/300 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9694692027421182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.10, cutpoint = ['-2.115', '-1.369', '-0.394', '0.488'] : 100%|█| 400/400 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9691596917232131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.09, cutpoint = ['-2.126', '-1.381', '-0.407', '0.473'] : 100%|█| 500/500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969049848280866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.09, cutpoint = ['-2.345', '-1.598', '-0.618', '0.267'] : 100%|█| 100/100 \n",
      "w0 = 0.08, cutpoint = ['-2.315', '-1.561', '-0.580', '0.307'] : 100%|█| 200/200 \n",
      "w0 = 0.07, cutpoint = ['-2.279', '-1.528', '-0.547', '0.341'] : 100%|█| 300/300 \n",
      "w0 = 0.07, cutpoint = ['-2.284', '-1.536', '-0.556', '0.331'] : 100%|█| 400/400 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9689563788004035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.06, cutpoint = ['-2.300', '-1.547', '-0.569', '0.319'] : 100%|█| 500/500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9688279922971721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = -0.02, cutpoint = ['-1.980', '-1.228', '-0.243', '0.646'] : 100%|█| 100/100\n",
      "w0 = -0.01, cutpoint = ['-1.960', '-1.212', '-0.225', '0.667'] : 100%|█| 200/200\n",
      "w0 = -0.01, cutpoint = ['-1.960', '-1.205', '-0.220', '0.669'] : 100%|█| 300/300\n",
      "w0 = 0.00, cutpoint = ['-1.959', '-1.202', '-0.215', '0.674'] : 100%|█| 400/400 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9685442803459683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.03, cutpoint = ['-1.970', '-1.218', '-0.233', '0.659'] : 100%|█| 500/500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683892296682076\n"
     ]
    }
   ],
   "source": [
    "# Return root mean square error metric\n",
    "def RMSE(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    return np.sqrt(np.average((x - y) ** 2))\n",
    "\n",
    "# Function to do parameter tuning\n",
    "def validation(user_encoder,movie_encoder,df_train,train_blocks,test_blocks,val_predictions):\n",
    "    group_shapes = user_encoder.encoder_shapes + movie_encoder.encoder_shapes\n",
    "\n",
    "    rank = [10,12,14,16]\n",
    "    n_iter = [100,200,300,400,500]\n",
    "    n_samples = [95,195,295,395,495]\n",
    "\n",
    "    best_rmse = 1\n",
    "    best_r = 0\n",
    "    best_n = 0\n",
    "    best_s = 0\n",
    "\n",
    "    for r in rank:\n",
    "        for i in range(len(n_iter)):\n",
    "            fm_probit = myfm.MyFMOrderedProbit(rank=r, random_seed=1234)\n",
    "            fm_probit.fit(None, df_train.rating.values - 1, X_rel=train_blocks,\n",
    "                          group_shapes=group_shapes,n_iter=n_iter[i], n_kept_samples=n_samples[i])\n",
    "            test_prediction_ordered_prob = fm_probit.predict_proba(None, test_blocks)\n",
    "            test_prediction_ordered_mean = 1 + test_prediction_ordered_prob.dot(np.arange(5)) # class 0 => rating 1 shift\n",
    "            rmse = RMSE(val_predictions,test_prediction_ordered_mean)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_r = r\n",
    "                best_n = n_iter[i]\n",
    "                best_s = n_samples[i]\n",
    "                print(best_rmse)\n",
    "                \n",
    "    return best_rmse,best_r,best_n,best_s\n",
    "\n",
    "best_rmse,best_r,best_n,best_s = validation(user_encoder,movie_encoder,df_train,train_blocks,test_blocks,val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028d376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE using model 0.9683892296682076\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation RMSE using model \" + str(best_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8c1102",
   "metadata": {},
   "source": [
    "# Generate Features & Train the model on Total Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa8cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w0 = 0.06, cutpoint = ['-1.994', '-1.237', '-0.252', '0.637'] : 100%|█| 500/500 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<myfm.gibbs.MyFMOrderedProbit at 0x29c941700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U,Q = surprise_svd(df_total)\n",
    "\n",
    "#Generate User Vectors\n",
    "user_vec = []\n",
    "for i in range(10000):\n",
    "    user_vec.append(dict(enumerate(np.array(U[i]).flatten(), 1)))\n",
    "\n",
    "#Generate Movie Vectors\n",
    "movie_vec = []\n",
    "for i in range(1000):\n",
    "    movie_vec.append(dict(enumerate(np.array(Q[i]).flatten(), 1)))\n",
    "\n",
    "user_f = pd.DataFrame(np.arange(10000),columns=['userID'])\n",
    "user_f['user_implicit_features'] = df_total.groupby('userID').movieID.agg(string_agg).reindex(user_f.index).fillna('')\n",
    "user_f['user_vec'] = user_vec\n",
    "\n",
    "\n",
    "\n",
    "movie_f = pd.DataFrame(np.arange(1000),columns=['movieID'])\n",
    "movie_f['movie_implicit_features'] = df_total.groupby('movieID').userID.agg(string_agg).reindex(movie_f.index).fillna('')\n",
    "movie_f['movie_vec'] = movie_vec\n",
    "\n",
    "#Create User Encoder\n",
    "user_encoder = DataFrameEncoder().add_column(\n",
    "    'userID', CategoryValueToSparseEncoder(user_f.userID)\n",
    "    )\n",
    "\n",
    "user_encoder.add_column(\n",
    "        'user_implicit_features',\n",
    "        MultipleValuesToSparseEncoder(user_f.user_implicit_features, normalize=True)\n",
    "    )\n",
    "\n",
    "user_encoder.add_column(\n",
    "        'user_vec',\n",
    "        ValuesToSparseEncoder(user_f.user_vec)\n",
    "    )\n",
    "\n",
    "#Create Movie Encoder\n",
    "movie_encoder = DataFrameEncoder().add_column(\n",
    "    'movieID', CategoryValueToSparseEncoder(movie_f.movieID)\n",
    "    )\n",
    "movie_encoder.add_column(\n",
    "        'movie_implicit_features',\n",
    "        MultipleValuesToSparseEncoder(movie_f.movie_implicit_features, normalize=True)\n",
    "    )\n",
    "movie_encoder.add_column(\n",
    "        'movie_vec',\n",
    "        ValuesToSparseEncoder(movie_f.movie_vec)\n",
    "    )\n",
    "\n",
    "\n",
    "def augment_user_id(user_ids):\n",
    "    return user_encoder.encode_df(\n",
    "        user_f.reindex(user_ids).reset_index()\n",
    "    )\n",
    "\n",
    "def augment_movie_id(movie_ids):\n",
    "    return movie_encoder.encode_df(\n",
    "        movie_f.reindex(movie_ids).reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "#Generate Train Blocks on total data\n",
    "group_shapes = user_encoder.encoder_shapes + movie_encoder.encoder_shapes\n",
    "\n",
    "total_train_blocks = []\n",
    "for source, target in [(df_total, total_train_blocks)]:\n",
    "    unique_users, user_map = np.unique(source.userID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(user_map, augment_user_id(unique_users))\n",
    "    )\n",
    "    unique_movies, movie_map = np.unique(source.movieID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(movie_map, augment_movie_id(unique_movies))\n",
    "    )\n",
    "\n",
    "\n",
    "#Train the model\n",
    "#optimal params: best_r=16, best_n=500, best_s=495\n",
    "fm_probit = myfm.MyFMOrderedProbit(rank=best_r, random_seed=1234)\n",
    "fm_probit.fit(None, df_total.rating.values - 1, X_rel=total_train_blocks,\n",
    "                          group_shapes=group_shapes,n_iter=best_n, n_kept_samples=best_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3dda0c",
   "metadata": {},
   "source": [
    "## Generate predictions for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a8fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Submission File\n",
    "sub_pd = pd.read_csv('sampleSubmission.csv')\n",
    "sub_users, sub_movies, sub_pred = extract_users_items_predictions(sub_pd)\n",
    "sub_test_ratings_dict = {'userID': sub_users,'movieID': sub_movies,'rating': sub_pred}\n",
    "sub_df = pd.DataFrame(sub_test_ratings_dict)\n",
    "\n",
    "#Generate Submission Test Blocks\n",
    "sub_test_blocks = []\n",
    "for source, target in [(sub_df, sub_test_blocks)]:\n",
    "    unique_users, user_map = np.unique(source.userID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(user_map, augment_user_id(unique_users))\n",
    "    )\n",
    "    unique_movies, movie_map = np.unique(source.movieID, return_inverse=True)\n",
    "    target.append(\n",
    "        RelationBlock(movie_map, augment_movie_id(unique_movies))\n",
    "    )\n",
    "\n",
    "#Generate Predictions and create submission csv    \n",
    "test_prediction_ordered_prob = fm_probit.predict_proba(None, sub_test_blocks)\n",
    "test_prediction_ordered_mean = 1 + test_prediction_ordered_prob.dot(np.arange(5))\n",
    "sub_pd['Prediction'] = test_prediction_ordered_mean\n",
    "sub_pd.set_index(\"Id\", inplace = True)\n",
    "sub_pd.to_csv(\"submission_bayesian_svdpp_flipped_embeddings_ordered_probit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
