{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Kernel Net is based on https://proceedings.mlr.press/v80/muller18a.html\n",
        "*   .py files are taken and adapted from https://github.com/lorenzMuller/kernelNet_MovieLens\n",
        "\n",
        "* this notebook was made to be ran on google collab \n",
        "\n"
      ],
      "metadata": {
        "id": "LRFkCpswW5FU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gh7s_5g6OYw"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "else:\n",
        "  print(\"GPU found\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J3SYDYbg8DrC"
      },
      "outputs": [],
      "source": [
        "def extract_users_items_predictions(data_pd):\n",
        "    users, movies = \\\n",
        "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
        "    predictions = data_pd.Prediction.values\n",
        "    return users, movies, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**specify data directory:** </br>\n",
        "*In order to use the notebook one simply has to create a folder, specificy the path to it and put the .py files of the kernelNet folder in the specified directory.*"
      ],
      "metadata": {
        "id": "LkprIFxRQ62H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FoLoC_jn9oO0"
      },
      "outputs": [],
      "source": [
        "#specify data directory and make sure required files are in the directory, despite of this nothing else has to be done to run the notebook\n",
        "DATA_DIR = '/content/kernelNet'\n",
        "\n",
        "assert os.path.isdir(DATA_DIR), \"directory does not exist\"\n",
        "assert os.path.exists(os.path.join(DATA_DIR, \"kernelNet_ml1m.py\")), \"upload kernelNet_ml1m.py to %s \" %(str(DATA_DIR))\n",
        "assert os.path.exists(os.path.join(DATA_DIR, \"dataLoader.py\")), \"upload dataLoader.py to %s \" %(str(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download training data and sample predictions \n",
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "import json\n",
        "\n",
        "kaggle_username = \"yuvalnis\" #@param {type:\"string\"}\n",
        "kaggle_api_key = \"1800d5a286834f0416c338c7bd7f6dee\" #@param {type:\"string\"}\n",
        "\n",
        "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
        "\n",
        "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
        "\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cil-collaborative-filtering-2022\n",
        "\n",
        "!unzip -n cil-collaborative-filtering-2022.zip\n",
        "\n",
        "os.rename(\"data_train.csv\", os.path.join(DATA_DIR,\"data_train.csv\"))\n",
        "os.rename(\"sampleSubmission.csv\", os.path.join(DATA_DIR,\"sampleSubmission.csv\"))\n",
        "\n",
        "!rm cil-collaborative-filtering-2022.zip"
      ],
      "metadata": {
        "cellView": "code",
        "id": "gqYRBgf9RAXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SSdKh-4m8M2R"
      },
      "outputs": [],
      "source": [
        "x_train_pd = pd.read_csv(os.path.join(DATA_DIR, \"data_train.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BVdWlTqH8O3c"
      },
      "outputs": [],
      "source": [
        "users,movies,predictions = extract_users_items_predictions(x_train_pd)\n",
        "ratings_dict = {'userID': users,'movieID': movies,'rating': predictions}\n",
        "df_train = pd.DataFrame(ratings_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"ratings.dat\"), df_train.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LuNlueIeJZOZ"
      },
      "outputs": [],
      "source": [
        "#load entries to predict \n",
        "to_predict_pd = pd.read_csv(os.path.join(DATA_DIR, \"sampleSubmission.csv\"))\n",
        "pred_users,pred_movies,pred_predictions = extract_users_items_predictions(to_predict_pd)\n",
        "to_predict_dict = {'userID': pred_users,'movieID': pred_movies,'rating': pred_predictions}\n",
        "df_predict = pd.DataFrame(to_predict_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"to_predict.dat\"), df_predict.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and prediction\n",
        "* input parameters: -file to execute -data directory -regularization 1 (default 60) -regularization 2 (default 0.013)\n",
        "* executing the file performs both training and prediction.\n",
        "* For the prediction the number of epochs with the lowest validation error is used"
      ],
      "metadata": {
        "id": "1FryctmrWKbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWNgRKwf9DRH"
      },
      "outputs": [],
      "source": [
        "file_to_execute = os.path.join(DATA_DIR,\"kernelNet_ml1m.py\")\n",
        "!python $file_to_execute $DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eOzS0ERtfhAM"
      },
      "outputs": [],
      "source": [
        "# i can throw  0'th column of raw predictions away \n",
        "raw_predictions = pd.read_csv(os.path.join(DATA_DIR, \"raw_predictions.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GtehjmGrErg"
      },
      "outputs": [],
      "source": [
        "output = to_predict_pd.to_numpy()\n",
        "final_predictions = raw_predictions.to_numpy()\n",
        "for id,user in enumerate(pred_users):\n",
        "  prediction = final_predictions[pred_movies[id]][user+1]\n",
        "  output[id][1] = prediction \n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV5eYBFhsSfM"
      },
      "outputs": [],
      "source": [
        "submission_df = pd.DataFrame(output, columns = ['Id', 'Prediction'])\n",
        "print(submission_df)\n",
        "try:\n",
        "    os.makedirs(os.path.join(DATA_DIR,\"final_predictions\"))\n",
        "except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass\n",
        "submission_df.to_csv(os.path.join(DATA_DIR,\"final_predictions/kernelNet.csv\"),index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "kernelNet.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}