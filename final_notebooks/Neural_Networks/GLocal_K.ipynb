{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLocal_K.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* GLocal-K is based on https://arxiv.org/pdf/2108.12184.pdf\n",
        "* this notebook is based on https://github.com/usydnlp/Glocal_K\n",
        "* this notebook was made to be ran on google collab\n",
        "\n"
      ],
      "metadata": {
        "id": "RXBWJYWv6yXS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.5\n",
        "import tensorflow as tf\n",
        "print(tf.VERSION)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "else:\n",
        "  print(\"GPU found\")\n",
        "  \n",
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import h5py\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and preprocess Kaggle Data"
      ],
      "metadata": {
        "id": "g9MQGYPLjPnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/GLocal_K'\n",
        "try:\n",
        "    os.makedirs(DATA_DIR)\n",
        "except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass"
      ],
      "metadata": {
        "id": "0ChfQ_onjNA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download training data and sample predictions\n",
        "#in case you can't download the data via this cell please put \"data_train.csv\"\n",
        "#and \"sampleSubmissions.csv\" in the DATA_DIR path defined above\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "import json\n",
        "\n",
        "kaggle_username = \"yuvalnis\" #@param {type:\"string\"}\n",
        "kaggle_api_key = \"1800d5a286834f0416c338c7bd7f6dee\" #@param {type:\"string\"}\n",
        "\n",
        "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
        "\n",
        "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
        "\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cil-collaborative-filtering-2022\n",
        "\n",
        "!unzip -n cil-collaborative-filtering-2022.zip\n",
        "\n",
        "os.rename(\"data_train.csv\", os.path.join(DATA_DIR,\"data_train.csv\"))\n",
        "os.rename(\"sampleSubmission.csv\", os.path.join(DATA_DIR,\"sampleSubmission.csv\"))\n",
        "\n",
        "!rm cil-collaborative-filtering-2022.zip"
      ],
      "metadata": {
        "cellView": "code",
        "id": "5JjTVKBtjTCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_users_items_predictions(data_pd):\n",
        "    users, movies = \\\n",
        "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
        "    predictions = data_pd.Prediction.values\n",
        "    return users, movies, predictions"
      ],
      "metadata": {
        "id": "zf35Ernnjqvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.read_csv(os.path.join(DATA_DIR, \"data_train.csv\"))\n",
        "users,movies,predictions = extract_users_items_predictions(x_train_pd)\n",
        "ratings_dict = {'userID': users,'movieID': movies,'rating': predictions}\n",
        "df_train = pd.DataFrame(ratings_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"ratings.dat\"), df_train.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ],
      "metadata": {
        "id": "sB4rnNZfjsQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load entries to predict \n",
        "to_predict_pd = pd.read_csv(os.path.join(DATA_DIR, \"sampleSubmission.csv\"))\n",
        "pred_users,pred_movies,pred_predictions = extract_users_items_predictions(to_predict_pd)\n",
        "to_predict_dict = {'userID': pred_users,'movieID': pred_movies,'rating': pred_predictions}\n",
        "df_predict = pd.DataFrame(to_predict_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"to_predict.dat\"), df_predict.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ],
      "metadata": {
        "id": "6u4y2ZcGjuH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3e8Xg3us8g7"
      },
      "source": [
        "def load_data(path='./', delimiter='::', frac=0.1, seed=1234, shuffle_data = True):\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    #data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    data = np.loadtxt(path, skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('taken', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:,0]).size  # num of users\n",
        "    n_m = np.unique(data[:,1]).size  # num of movies\n",
        "    n_r = data.shape[0]  # num of ratings\n",
        "\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    idx = np.arange(n_r)\n",
        "    if(shuffle_data):\n",
        "      np.random.shuffle(idx)\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        if i < int(frac * n_r):\n",
        "            test_r[mdict[m_id], udict[u_id]] = r\n",
        "        else:\n",
        "            train_r[mdict[m_id], udict[u_id]] = r\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
        "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = DATA_DIR\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX2wREO09zde"
      },
      "source": [
        "def local_kernel(u, v):\n",
        "\n",
        "    dist = tf.norm(u - v, ord=2, axis=2)\n",
        "    hat = tf.maximum(0., 1. - dist**2)\n",
        "\n",
        "    return hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c88l9LYr9175"
      },
      "source": [
        "def kernel_layer(x, n_hid, n_dim, lambda_s, lambda_2, activation=tf.nn.sigmoid, name=''):\n",
        "\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
        "        n_in = x.get_shape().as_list()[1]\n",
        "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
        "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
        "        b = tf.get_variable('b', [n_hid])\n",
        "\n",
        "    w_hat = local_kernel(u, v)\n",
        "    \n",
        "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
        "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
        "    \n",
        "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
        "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
        "\n",
        "    W_eff = W * w_hat  # Local kernelised weight matrix\n",
        "    y = tf.matmul(x, W_eff) + b\n",
        "    y = activation(y)\n",
        "\n",
        "    return y, sparse_reg_term + l2_reg_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlb95FmRVATa"
      },
      "source": [
        "def global_kernel(input, gk_size, dot_scale):\n",
        "\n",
        "    avg_pooling = tf.reduce_mean(input, axis=1)  # Item (axis=1) based average pooling\n",
        "    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n",
        "    n_kernel = avg_pooling.shape[1].value\n",
        "\n",
        "    conv_kernel = tf.get_variable('conv_kernel', initializer=tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1))\n",
        "    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n",
        "    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n",
        "\n",
        "    return gk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTLi_65XzIbH"
      },
      "source": [
        "def global_conv(input, W):\n",
        "\n",
        "    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n",
        "    conv2d = tf.nn.relu(tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME'))\n",
        "\n",
        "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code"
      ],
      "metadata": {
        "id": "sETwz58aK6y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ],
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ],
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ],
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Hyper-parameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ35Zoha-Eue"
      },
      "source": [
        "use_gpu = True\n",
        "\n",
        "#create logging dictionary\n",
        "logging_dict = {\"n_dim\": [],\n",
        "    \"n_hid\": [],\n",
        "    \"lambda_2\":[],\n",
        "    \"lambda_s\":[],\n",
        "    \"n_layers\": [],\n",
        "    \"gk_size\":[],\n",
        "    \"iter_p\":[],\n",
        "    \"iter_f\":[],\n",
        "    \"epoch_p\":[],\n",
        "    \"epoch_f\":[],\n",
        "    \"dot_scale\":[],\n",
        "    \"rmse\":[]\n",
        "}\n",
        "\n",
        "#define parameter grid \n",
        "\n",
        "param_grid= {\n",
        "    \"n_dim\": [5],\n",
        "    \"n_hid\": [500],\n",
        "    \"lambda_2\":[20.0],\n",
        "    \"lambda_s\":[0.006],\n",
        "    \"n_layers\": [2],\n",
        "    \"gk_size\":[3],\n",
        "    \"iter_p\":[5],\n",
        "    \"iter_f\":[5],\n",
        "    \"epoch_p\":[30],\n",
        "    \"epoch_f\":[60],\n",
        "    \"dot_scale\":[1.0]\n",
        "}\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "path = os.path.join(DATA_DIR, \"ratings.dat\")\n",
        "n_m, n_u, train_r, train_m, test_r, test_m = load_data(path=path, delimiter='::', frac=0.1, seed=1234)\n",
        "\n",
        "for params_dict in tqdm(permutations_dicts):\n",
        "\n",
        "  #reset tensorflow graph\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  #set hyperparameters\n",
        "  n_hid = int(params_dict[\"n_hid\"])\n",
        "  n_dim = int(params_dict[\"n_dim\"])\n",
        "  n_layers = int(params_dict[\"n_layers\"])\n",
        "  gk_size = int(params_dict[\"gk_size\"])\n",
        "  #advanced hyper params \n",
        "  lambda_2 = float(params_dict[\"lambda_2\"])\n",
        "  lambda_s = float(params_dict[\"lambda_s\"])\n",
        "  iter_p = int(params_dict[\"iter_p\"])\n",
        "  iter_f = int(params_dict[\"iter_f\"])\n",
        "  epoch_p = int(params_dict[\"epoch_p\"])\n",
        "  epoch_f = int(params_dict[\"epoch_f\"])\n",
        "  dot_scale = float(params_dict[\"dot_scale\"])\n",
        "\n",
        "\n",
        "\n",
        "  #input placeholders\n",
        "  R = tf.placeholder(\"float\", [n_m, n_u])\n",
        "  #build model pre-training \n",
        "  y = R\n",
        "  reg_losses = None\n",
        "\n",
        "  for i in range(n_layers):\n",
        "      y, reg_loss = kernel_layer(y, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "      reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "  pred_p, reg_loss = kernel_layer(y, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2,  activation=tf.identity, name='out')\n",
        "  reg_losses = reg_losses + reg_loss\n",
        "\n",
        "  # L2 loss\n",
        "  diff = train_m * (train_r - pred_p)\n",
        "  sqE = tf.nn.l2_loss(diff)\n",
        "  loss_p = sqE + reg_losses\n",
        "\n",
        "  optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B') \n",
        "\n",
        "  #build model fine-tuning\n",
        "\n",
        "  y = R\n",
        "  reg_losses = None\n",
        "\n",
        "  for i in range(n_layers):\n",
        "      y, _ = kernel_layer(y, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "\n",
        "  y_dash, _ = kernel_layer(y, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, activation=tf.identity, name='out')\n",
        "\n",
        "  gk = global_kernel(y_dash, gk_size, dot_scale)  # Global kernel\n",
        "  y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n",
        "\n",
        "  for i in range(n_layers):\n",
        "      y_hat, reg_loss = kernel_layer(y_hat, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "      reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "  pred_f, reg_loss = kernel_layer(y_hat, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, activation=tf.identity, name='out')\n",
        "  reg_losses = reg_losses + reg_loss\n",
        "\n",
        "  # L2 loss\n",
        "  diff = train_m * (train_r - pred_f)\n",
        "  sqE = tf.nn.l2_loss(diff)\n",
        "  loss_f = sqE + reg_losses\n",
        "\n",
        "  optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "  best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "  time_cumulative = 0\n",
        "\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      for i in tqdm(range(epoch_p)):\n",
        "          tic = time()\n",
        "          optimizer_p.minimize(sess, feed_dict={R: train_r})\n",
        "          pre = sess.run(pred_p, feed_dict={R: train_r})\n",
        "\n",
        "          t = time() - tic\n",
        "          time_cumulative += t\n",
        "          \n",
        "          error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "          test_rmse = np.sqrt(error)\n",
        "\n",
        "          error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "          train_rmse = np.sqrt(error_train)\n",
        "\n",
        "          print('.-^-._' * 12)\n",
        "          print('PRE-TRAINING')\n",
        "          print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "          print('Time:', t, 'seconds')\n",
        "          print('Time cumulative:', time_cumulative, 'seconds')\n",
        "          print('.-^-._' * 12)\n",
        "\n",
        "      for i in tqdm(range(epoch_f)):\n",
        "          tic = time()\n",
        "          optimizer_f.minimize(sess, feed_dict={R: train_r})\n",
        "          pre = sess.run(pred_f, feed_dict={R: train_r})\n",
        "\n",
        "          t = time() - tic\n",
        "          time_cumulative += t\n",
        "          \n",
        "          error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "          test_rmse = np.sqrt(error)\n",
        "\n",
        "          error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "          train_rmse = np.sqrt(error_train)\n",
        "\n",
        "          test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "          train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "          test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "          train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "          if test_rmse < best_rmse:\n",
        "              best_rmse = test_rmse\n",
        "              best_rmse_ep = i+1\n",
        "\n",
        "          if test_mae < best_mae:\n",
        "              best_mae = test_mae\n",
        "              best_mae_ep = i+1\n",
        "\n",
        "          if best_ndcg < test_ndcg:\n",
        "              best_ndcg = test_ndcg\n",
        "              best_ndcg_ep = i+1\n",
        "\n",
        "          print('.-^-._' * 12)\n",
        "          print('FINE-TUNING')\n",
        "          print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "          print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "          print('Time:', t, 'seconds')\n",
        "          print('Time cumulative:', time_cumulative, 'seconds')\n",
        "          print('.-^-._' * 12)\n",
        "\n",
        "      # Final result\n",
        "      print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "      print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "      print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)\n",
        "\n",
        "      #log_hyperparameters\n",
        "      logging_dict[\"n_dim\"].append(n_dim);\n",
        "      logging_dict[\"n_hid\"].append(n_hid);\n",
        "      logging_dict[\"lambda_2\"].append(lambda_2);\n",
        "      logging_dict[\"lambda_s\"].append(lambda_s);\n",
        "      logging_dict[\"n_layers\"].append(n_layers);\n",
        "      logging_dict[\"gk_size\"].append(gk_size);\n",
        "      logging_dict[\"iter_p\"].append(iter_p)\n",
        "      logging_dict[\"iter_f\"].append(iter_f)\n",
        "      logging_dict[\"epoch_p\"].append(epoch_p)\n",
        "      logging_dict[\"epoch_f\"].append(best_rmse_ep)\n",
        "      logging_dict[\"dot_scale\"].append(dot_scale)\n",
        "      #val rmse\n",
        "      logging_dict[\"rmse\"].append(best_rmse)\n",
        "      \n",
        "      #import current log to csv\n",
        "      log_df = pd.DataFrame.from_dict(logging_dict)\n",
        "      log_df.sort_values(\"rmse\", inplace = True)\n",
        "      log_df.to_csv(os.path.join(DATA_DIR,\"log_df.csv\"),index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_df = pd.read_csv(os.path.join(DATA_DIR, \"log_df.csv\"))\n",
        "print(f\"Table with the results of the parameter tuning:\")\n",
        "display(log_df)"
      ],
      "metadata": {
        "id": "CwKUJHKdxndj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions using optimal hyper-parameters"
      ],
      "metadata": {
        "id": "WwXTnp_8ojp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train again on full data set with optimal hyper-parameters**"
      ],
      "metadata": {
        "id": "j3GtZiSnopzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = os.path.join(DATA_DIR, \"ratings.dat\")\n",
        "n_m, n_u, train_r, train_m, test_r, test_m = load_data(path=path, delimiter='::', frac = 0.0, seed=1234)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#setup models with best performing hyperparamters (log_df was sorted by ascending validation rmse)\n",
        "log_df = pd.read_csv(os.path.join(DATA_DIR, \"log_df.csv\"))\n",
        "best_params = log_df.iloc[[0]]\n",
        "\n",
        "n_hid = int(best_params[\"n_hid\"].values[0])\n",
        "n_dim = int(best_params[\"n_dim\"].values[0])\n",
        "n_layers = int(best_params[\"n_layers\"].values[0])\n",
        "gk_size = int(best_params[\"gk_size\"].values[0])\n",
        "#advanced hyper params \n",
        "lambda_2 = float(best_params[\"lambda_2\"].values[0])\n",
        "lambda_s = float(best_params[\"lambda_s\"].values[0])\n",
        "iter_p = int(best_params[\"iter_p\"].values[0])\n",
        "iter_f = int(best_params[\"iter_f\"].values[0])\n",
        "epoch_p = int(best_params[\"epoch_p\"].values[0])\n",
        "epoch_f = int(best_params[\"epoch_f\"].values[0])\n",
        "dot_scale = float(best_params[\"dot_scale\"].values[0])\n",
        "\n",
        "\n",
        "#input placeholders\n",
        "R = tf.placeholder(\"float\", [n_m, n_u])\n",
        "#build model pre-training \n",
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, reg_loss = kernel_layer(y, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_p, reg_loss = kernel_layer(y, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2,  activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_p)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_p = sqE + reg_losses\n",
        "\n",
        "optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B') \n",
        "\n",
        "#build model fine-tuning\n",
        "\n",
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, _ = kernel_layer(y, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "\n",
        "y_dash, _ = kernel_layer(y, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, activation=tf.identity, name='out')\n",
        "\n",
        "gk = global_kernel(y_dash, gk_size, dot_scale)  # Global kernel\n",
        "y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y_hat, reg_loss = kernel_layer(y_hat, n_hid = n_hid, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_f, reg_loss = kernel_layer(y_hat, n_hid = n_u, n_dim = n_dim,lambda_s = lambda_s, lambda_2 = lambda_2, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_f)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_f = sqE + reg_losses\n",
        "\n",
        "optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')\n",
        "\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in tqdm(range(epoch_p)):\n",
        "        optimizer_p.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_p, feed_dict={R: train_r})\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('PRE-TRAINING')\n",
        "        print('Epoch:', i+1, 'train rmse:', train_rmse)\n",
        "        print('.-^-._' * 12)\n",
        "\n",
        "    for i in tqdm(range(epoch_f)):\n",
        "        optimizer_f.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_f, feed_dict={R: train_r})\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "        train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('FINE-TUNING')\n",
        "        print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "        print('.-^-._' * 12)\n",
        "\n",
        "    print(\"finished training on whole training data\")\n",
        "\n",
        "    #make raw predictions of our data \n",
        "    path = os.path.join(DATA_DIR, \"to_predict.dat\")\n",
        "    n_m, n_u, train_r, train_m, test_r, test_m = load_data(path=path, delimiter='::', frac = 0.0, seed=1234, shuffle_data = False)\n",
        "\n",
        "    pre = sess.run(pred_p, feed_dict={R: train_r})\n",
        "    pd.DataFrame(pre).to_csv(os.path.join(DATA_DIR, \"raw_predictions.csv\"))\n",
        "\n",
        "    print(\"finished raw predictions\")"
      ],
      "metadata": {
        "id": "GI3uwngCos0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i can throw  0'th column of raw predictions away as it is just the index\n",
        "raw_predictions = pd.read_csv(os.path.join(DATA_DIR, \"raw_predictions.csv\"))\n",
        "#convert raw predictions to correct format\n",
        "output = to_predict_pd.to_numpy()\n",
        "final_predictions = raw_predictions.to_numpy()\n",
        "for id,user in enumerate(pred_users):\n",
        "  prediction = final_predictions[pred_movies[id]][user+1]\n",
        "  output[id][1] = prediction"
      ],
      "metadata": {
        "id": "MTuiXL6dqmjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame(output, columns = ['Id', 'Prediction'])\n",
        "try:\n",
        "    os.makedirs(os.path.join(DATA_DIR,\"final_predictions\"))\n",
        "except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass\n",
        "submission_df.to_csv(os.path.join(DATA_DIR,\"final_predictions/GLocal_K.csv\"),index = False)"
      ],
      "metadata": {
        "id": "QBv924Xzqnle"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}