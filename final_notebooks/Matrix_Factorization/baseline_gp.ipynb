{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline method with normalization using Gaussian processes"
      ],
      "metadata": {
        "id": "pHaLaM3az6Ni"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DboqRCo_NhwK"
      },
      "source": [
        "In this notebook the baseline methiod (normalization -> SVD -> ALS -> denormalization) is extended by adding a initialization of unobserved ratings using Gaussian processes.\n",
        "\n",
        "The sections \"Preprocess\", \"Run baseline + Gaussion processes method\" and \"Create submission file\" can be run entirely and in sequence to produce the results. Run the \"Download data from Kaggle\" code block only if you are running on Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download data from Kaggle"
      ],
      "metadata": {
        "id": "LH8C1GHkzvSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "import json\n",
        "\n",
        "kaggle_username = \"yuvalnis\" #@param {type:\"string\"}\n",
        "kaggle_api_key = \"1800d5a286834f0416c338c7bd7f6dee\" #@param {type:\"string\"}\n",
        "\n",
        "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
        "\n",
        "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
        "\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cil-collaborative-filtering-2022\n",
        "\n",
        "!unzip -n cil-collaborative-filtering-2022.zip"
      ],
      "metadata": {
        "id": "BIOHImfzzyT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vt7acoKM8wP"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUIuYSCENCk1"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir8e-ogLMS0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyVtWL1vCipD"
      },
      "source": [
        "### Declare global parameters of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUfftTI_Cft1"
      },
      "outputs": [],
      "source": [
        "total_num_users = 10000\n",
        "total_num_movies = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogmuKPI_UzVc"
      },
      "source": [
        "### Data parsing helper function declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm4_OA6-VPJw"
      },
      "outputs": [],
      "source": [
        "def parse_csv(csv_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "  Extract the arrays of user indices, item indices and ratings listed in a .csv file\n",
        "\n",
        "  :param csv_path: path to .csv file to read from\n",
        "  :return: 3 arrays containing the users indices, the item indices and the observed ratings in order  \n",
        "  \"\"\"\n",
        "  df = pd.read_csv(csv_path)\n",
        "  # extract user and item indices from the Id label in the dataframe\n",
        "  df = df.join(df.Id.str.extract(r\"r(?P<User>\\d+)_c(?P<Item>\\d+)\").astype(int) - 1)\n",
        "  # extract user, item and prediction triplets from dataframe\n",
        "  users = df.User.values\n",
        "  items = df.Item.values\n",
        "  preds = df.Prediction.values\n",
        "  return users, items, preds\n",
        "\n",
        "def construct_ratings_matrix(\n",
        "    users: np.ndarray,\n",
        "    items: np.ndarray,\n",
        "    ratings: np.ndarray,\n",
        "    n_users: int,\n",
        "    n_items: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "  Constructs the ratings matrix with NaN values where no rating was observed\n",
        "\n",
        "  :param users: array of user indices\n",
        "  :param items: array of item indices\n",
        "  :param predictions: array of ratings per user-item pair\n",
        "  :param n_users: total number of users\n",
        "  :param n_items: total number of items\n",
        "  :return: the ratings matrix and the observed ratings mask\n",
        "  \"\"\"\n",
        "  ratings_matrix = np.zeros((n_users, n_items))\n",
        "  observed_mask = np.full((n_users, n_items), fill_value=False)\n",
        "  for r, c, v in zip(users, items, ratings):\n",
        "    observed_mask[r, c] = True\n",
        "    ratings_matrix[r][c] = v\n",
        "\n",
        "  ratings_matrix[~observed_mask] = np.nan\n",
        "\n",
        "  return ratings_matrix, observed_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pXPEMw24bAY"
      },
      "source": [
        "### Model function declarations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKwuoKmfORj9"
      },
      "outputs": [],
      "source": [
        "def KAA_splicer(covmat, selection_A):\n",
        "  assert(selection_A.shape[0] == covmat.shape[0])\n",
        "  assert(selection_A.shape[0] == covmat.shape[1])\n",
        "  temp = covmat[:, selection_A]\n",
        "  temp = temp[selection_A, :]\n",
        "  return temp\n",
        "\n",
        "def KXA_splicer(covmat, selection_A):\n",
        "  assert(selection_A.shape[0] == covmat.shape[0])\n",
        "  assert(selection_A.shape[0] == covmat.shape[1])\n",
        "  temp = covmat[:, selection_A]\n",
        "  temp = temp[np.logical_not(selection_A), :]\n",
        "  return temp\n",
        "\n",
        "def MUA_splicer(mu, selection_A):\n",
        "  assert(selection_A.shape[0] == mu.shape[0])\n",
        "  temp = mu[np.logical_not(selection_A)]\n",
        "  return temp\n",
        "\n",
        "def gauss_kernel(a, b, bandwidth):\n",
        "  return np.exp(-(np.linalg.norm(a-b)**2)/bandwidth**2)\n",
        "\n",
        "#X is the recommender system Matrix and entry_mask masks the existend entries\n",
        "def gp_imputation(X, bandwidth, noise):\n",
        "  #Xcombined = np.concatenate((X, Xtest))\n",
        "  Xcombined = X\n",
        "  maskedXcombined = np.ma.array(Xcombined, mask = np.logical_not(np.isnan(Xcombined)))\n",
        "  mean = np.nanmean(Xcombined, axis = 0)\n",
        "  cov = np.zeros((Xcombined.shape[1], Xcombined.shape[1]))\n",
        "  notnan_mask = np.logical_not(np.isnan(Xcombined))\n",
        "  for i in range(Xcombined.shape[1]):\n",
        "    localmask = notnan_mask[:, i]\n",
        "    #cov[i, i] = (np.matmul(Xcombined[localmask, i], Xcombined[localmask, i])**2+1)**2\n",
        "    cov[i, i] = np.var(Xcombined[localmask, i])\n",
        "    #cov[i, i] = 1.0\n",
        "    for j in range(i + 1, Xcombined.shape[1]):\n",
        "      localmask = notnan_mask[:, i] & notnan_mask[:, j]\n",
        "      t1 = Xcombined[localmask, i]\n",
        "      t2 = Xcombined[localmask, j]\n",
        "      #temp = np.cov([t1, t2])\n",
        "      #cov[i, j] = (np.matmul(t1, t2) + 1)**2\n",
        "      #cov[i, j] = temp[0, 1]\n",
        "      cov[i, j] = gauss_kernel(t1, t2, bandwidth)\n",
        "      cov[j, i] = cov[i, j]\n",
        "  Xcombined_new = np.copy(Xcombined)\n",
        "\n",
        "  print(Xcombined.shape)\n",
        "\n",
        "  for index, features in enumerate(Xcombined):\n",
        "    selection_A = notnan_mask[index, :]\n",
        "    mu = MUA_splicer(mean, selection_A)\n",
        "    kxa = KXA_splicer(cov, selection_A)\n",
        "    kaa = KAA_splicer(cov, selection_A)\n",
        "    ya = Xcombined[index, selection_A]\n",
        "    #print(mu.shape)\n",
        "    #print(kxa.shape)\n",
        "    #print(kaa.shape)\n",
        "    #print(ya.T.shape)\n",
        "    pred = mu + np.matmul(kxa, np.linalg.solve(kaa + np.identity(kaa.shape[0]) * noise**2, ya.T))\n",
        "    #print(pred)\n",
        "    #assert(False)\n",
        "    for i in range(Xcombined.shape[1]):\n",
        "      curr = 0\n",
        "      if np.logical_not(notnan_mask[index, i]):\n",
        "        Xcombined_new[index, i] = pred[curr]\n",
        "        curr += 1\n",
        "  #assert(not np.any(np.isnan(Xcombined_new)))\n",
        "  return Xcombined_new\n",
        "\n",
        "\n",
        "def normalize(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "  Normalizes the input data matrix per item (column). NaN entries are disregarded and are set using Gaussian processes.  \n",
        "\n",
        "  :param data: matrix to normalize\n",
        "  :return: the normalized input matrix, the column-wise mean and the column-wise standard deviation\n",
        "  \"\"\"\n",
        "  # compute mean and std and normalized the matrix with NaN values\n",
        "  mean = np.nanmean(data, axis=0, keepdims=True)\n",
        "  std = np.nanstd(data, axis=0, keepdims=True)\n",
        "  normed_data = (data - mean) / std\n",
        "  # set the non-observed entries to 0\n",
        "  normed_data = gp_imputation(normed_data, 0.1, 1.0)\n",
        "  return normed_data, mean, std\n",
        "\n",
        "\n",
        "def denormalize(data: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Denormalizes the input data matrix per item (column) by performing the inverse operation of the normalize function  \n",
        "\n",
        "  :param data: matrix to denormalize\n",
        "  :param mean: the column-wise mean of the unnormalized matrix\n",
        "  :param std: the column-wise std of the unnormalized matrix\n",
        "  :return: the denormalized matrix\n",
        "  \"\"\"\n",
        "  return (data * std) + mean\n",
        "\n",
        "\n",
        "def SVD(A: np.ndarray, k: int) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Computes the singular value decomposition of the input matrix, projected on\n",
        "  the subspace defined by the k largest singular values, as a 3-tuple\n",
        "\n",
        "  :param A: matrix to decompose\n",
        "  :param k: number of the largest singular values\n",
        "  :return: the projected singular value decomposition of A\n",
        "  \"\"\"\n",
        "  assert(k <= min(A.shape)), \"k should be no greater than the number of users or items.\"\n",
        "  U, S, VT = np.linalg.svd(A, full_matrices=False)\n",
        "  return U[:, :k], np.diag(S[:k]), VT[:k, :]\n",
        "\n",
        "\n",
        "def RMSE(x: np.ndarray, y: np.ndarray, mask: np.ndarray) -> float:\n",
        "  return np.sqrt(np.nansum(mask * (x - y) ** 2) / np.sum(mask))\n",
        "\n",
        "\n",
        "def ALS(\n",
        "    data: np.ndarray,\n",
        "    mask: np.ndarray,\n",
        "    U: np.ndarray,\n",
        "    VT: np.ndarray,\n",
        "    n_latent_factors: int,\n",
        "    regularization_param: float,\n",
        "    n_iterations: int\n",
        ") -> np.ndarray:\n",
        "  regularizer = regularization_param * np.eye(n_latent_factors)\n",
        "\n",
        "  with tqdm(total=n_iterations * (np.sum(mask.shape))) as pbar:\n",
        "    rmse_score = RMSE(data, U @ VT, mask)\n",
        "    pbar.set_description(f\"Initial RMSE score is {rmse_score:.4f}\")\n",
        "    for iter in range(n_iterations):\n",
        "      for i, Ri in enumerate(mask):\n",
        "        U[i] = np.linalg.solve(\n",
        "            np.dot(VT, np.dot(np.diag(Ri), VT.T)) + regularizer,\n",
        "            (np.dot(VT, np.dot(np.diag(Ri), data[i].T))).T\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "      for j, Rj in enumerate(mask.T):\n",
        "        VT[:,j] = np.linalg.solve(\n",
        "            np.dot(U.T, np.dot(np.diag(Rj), U)) + regularizer,\n",
        "            np.dot(U.T, np.dot(np.diag(Rj), data[:, j]))\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "      rmse_score = RMSE(data, U @ VT, mask)\n",
        "      pbar.set_description(f\"At iteration #{iter + 1} the RMSE score is {rmse_score:.4f}\")\n",
        "\n",
        "  return U @ VT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsqr8_Wmzuc4"
      },
      "source": [
        "## Run baseline + Gaussion processes method"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_latent_factors = 3\n",
        "regularization_param = 0.1\n",
        "n_iterations = 20"
      ],
      "metadata": {
        "id": "jBMxeA5-yznC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users, items, preds = parse_csv('data_train.csv')\n",
        "ratings, mask = construct_ratings_matrix(users, items, preds, total_num_users, total_num_movies)\n",
        "norm_data, data_mean, data_std = normalize(ratings)\n",
        "U, _, VT = SVD(norm_data, n_latent_factors)\n",
        "normed_pred_ratings = ALS(norm_data, mask, U, VT, n_latent_factors, regularization_param, n_iterations)\n",
        "pred_ratings = denormalize(normed_pred_ratings, data_mean, data_std)\n",
        "print(f\"Validation RMSE: {RMSE(pred_ratings, ratings, mask)}\")"
      ],
      "metadata": {
        "id": "QNhcTvuib1UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI6Me2qi4mc7"
      },
      "source": [
        "## Create submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3eDJ2Kz2LvQ"
      },
      "outputs": [],
      "source": [
        "submission_users, submission_items, _ = parse_csv('sampleSubmission.csv')\n",
        "df = pd.DataFrame({\n",
        "    \"Id\": [f\"r{r + 1}_c{c + 1}\" for r, c in zip(submission_users, submission_items)],\n",
        "    \"Prediction\": [pred_ratings[r, c] for r, c in zip(submission_users, submission_items)]\n",
        "})\n",
        "df.to_csv(f\"baseline_gp_reg-{regularization_param:.2f}_k-{n_latent_factors}_iters-{n_iterations}_submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "baseline_gp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}