{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kernelNetFinal.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Kernel Net is based on https://proceedings.mlr.press/v80/muller18a.html\n",
        "*   this notebook is based on https://github.com/lorenzMuller/kernelNet_MovieLens\n",
        "* this notebook was made to be ran on google collab\n",
        "\n"
      ],
      "metadata": {
        "id": "uX0v0dp6q6QM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "WiIo3Y9o2OfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_uatJk17dq",
        "outputId": "6ad422d8-919c-4d6b-db77-91b5ab88fa09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU found\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "else:\n",
        "  print(\"GPU found\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import os\n",
        "import sys\n",
        "from time import time\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Seed"
      ],
      "metadata": {
        "id": "MKBWdVYedzxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = int(time())\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "LdR5Hdqgd0hf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and preprocess Kaggle Data"
      ],
      "metadata": {
        "id": "ou6fHdLC23c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = '/content/kernelNet'\n",
        "try:\n",
        "    os.makedirs(DATA_DIR)\n",
        "except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass"
      ],
      "metadata": {
        "id": "gneKjeLr3COM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download training data and sample predictions \n",
        "!pip install kaggle\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "import json\n",
        "\n",
        "kaggle_username = \"yuvalnis\" #@param {type:\"string\"}\n",
        "kaggle_api_key = \"1800d5a286834f0416c338c7bd7f6dee\" #@param {type:\"string\"}\n",
        "\n",
        "assert len(kaggle_username) > 0 and len(kaggle_api_key) > 0\n",
        "\n",
        "api_token = {\"username\": kaggle_username,\"key\": kaggle_api_key}\n",
        "\n",
        "with open('kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!mv kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cil-collaborative-filtering-2022\n",
        "\n",
        "!unzip -n cil-collaborative-filtering-2022.zip\n",
        "\n",
        "os.rename(\"data_train.csv\", os.path.join(DATA_DIR,\"data_train.csv\"))\n",
        "os.rename(\"sampleSubmission.csv\", os.path.join(DATA_DIR,\"sampleSubmission.csv\"))\n",
        "\n",
        "!rm cil-collaborative-filtering-2022.zip"
      ],
      "metadata": {
        "cellView": "code",
        "id": "-yUn9G9p3tf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_users_items_predictions(data_pd):\n",
        "    users, movies = \\\n",
        "        [np.squeeze(arr) for arr in np.split(data_pd.Id.str.extract('r(\\d+)_c(\\d+)').values.astype(int) - 1, 2, axis=-1)]\n",
        "    predictions = data_pd.Prediction.values\n",
        "    return users, movies, predictions"
      ],
      "metadata": {
        "id": "x8DeNxOj34b9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_pd = pd.read_csv(os.path.join(DATA_DIR, \"data_train.csv\"))\n",
        "users,movies,predictions = extract_users_items_predictions(x_train_pd)\n",
        "ratings_dict = {'userID': users,'movieID': movies,'rating': predictions}\n",
        "df_train = pd.DataFrame(ratings_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"ratings.dat\"), df_train.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ],
      "metadata": {
        "id": "GryvQ13X37wM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load entries to predict \n",
        "to_predict_pd = pd.read_csv(os.path.join(DATA_DIR, \"sampleSubmission.csv\"))\n",
        "pred_users,pred_movies,pred_predictions = extract_users_items_predictions(to_predict_pd)\n",
        "to_predict_dict = {'userID': pred_users,'movieID': pred_movies,'rating': pred_predictions}\n",
        "df_predict = pd.DataFrame(to_predict_dict) \n",
        "np.savetxt(os.path.join(DATA_DIR, \"to_predict.dat\"), df_predict.values, delimiter='::', fmt='%s',encoding='utf-8')"
      ],
      "metadata": {
        "id": "oX1_-j1s3_np"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define data loading function for model "
      ],
      "metadata": {
        "id": "v4C14ui14VUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(path='./', valfrac=0.1, delimiter='::', seed=1234,\n",
        "             transpose=False, shuffle_data = True):\n",
        "    '''\n",
        "    loads kaggle data \n",
        "    :param path: path to the ratings file\n",
        "    :param valfrac: fraction of data to use for validation\n",
        "    :param delimiter: delimiter used in data file\n",
        "    :param seed: random seed for validation splitting\n",
        "    :param transpose: flag to transpose output matrices (swapping users with movies)\n",
        "    :return: train ratings (n_u, n_m), valid ratings (n_u, n_m)\n",
        "    '''\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    data = np.loadtxt(path, skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('data read in', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:, 0]).shape[0]  # number of users\n",
        "    n_m = np.unique(data[:, 1]).shape[0]  # number of movies\n",
        "    n_r = data.shape[0]  # number of ratings\n",
        "\n",
        "    # these dictionaries define a mapping from user/movie id to to user/movie number (contiguous from zero)\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    # shuffle indices\n",
        "    idx = np.arange(n_r)\n",
        "    if(shuffle_data):\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "    trainRatings = np.zeros((n_u, n_m), dtype='float32')\n",
        "    validRatings = np.zeros((n_u, n_m), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        # the first few ratings of the shuffled data array are validation data\n",
        "        if i <= valfrac * n_r:\n",
        "            validRatings[udict[u_id], mdict[m_id]] = int(r)\n",
        "        # the rest are training data\n",
        "        else:\n",
        "            trainRatings[udict[u_id], mdict[m_id]] = int(r)\n",
        "\n",
        "    if transpose:\n",
        "        trainRatings = trainRatings.T\n",
        "        validRatings = validRatings.T\n",
        "\n",
        "    print('loaded dense data matrix')\n",
        "\n",
        "    return trainRatings, validRatings"
      ],
      "metadata": {
        "id": "wn5aOEfP4NF-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "l4CB0Vlj5bL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define network functions\n",
        "def kernel(u, v):\n",
        "    \"\"\"\n",
        "    Sparsifying kernel function\n",
        "    :param u: input vectors [n_in, 1, n_dim]\n",
        "    :param v: output vectors [1, n_hid, n_dim]\n",
        "    :return: input to output connection matrix\n",
        "    \"\"\"\n",
        "    dist = tf.norm(u - v, ord=2, axis=2)\n",
        "    hat = tf.maximum(0., 1. - dist**2)\n",
        "    return hat\n",
        "\n",
        "\n",
        "def kernel_layer(x, n_hid, n_dim, lambda_s,\n",
        "                 lambda_2, activation=tf.nn.sigmoid, name=''):\n",
        "    \"\"\"\n",
        "    a kernel sparsified layer\n",
        "    :param x: input [batch, channels]\n",
        "    :param n_hid: number of hidden units\n",
        "    :param n_dim: number of dimensions to embed for kernelization\n",
        "    :param activation: output activation\n",
        "    :param name: layer name for scoping\n",
        "    :return: layer output, regularization term\n",
        "    \"\"\"\n",
        "\n",
        "    # define variables\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
        "        n_in = x.get_shape().as_list()[1]\n",
        "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
        "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
        "        b = tf.get_variable('b', [n_hid])\n",
        "\n",
        "    # compute sparsifying kernel\n",
        "    # as u and v move further from each other for some given pair of neurons, their connection\n",
        "    # decreases in strength and eventually goes to zero.\n",
        "    w_hat = kernel(u, v)\n",
        "\n",
        "    # compute regularization terms\n",
        "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
        "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
        "\n",
        "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
        "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
        "\n",
        "    # compute output\n",
        "    W_eff = W * w_hat\n",
        "    y = tf.matmul(x, W_eff) + b\n",
        "    y = activation(y)\n",
        "    return y, sparse_reg_term + l2_reg_term"
      ],
      "metadata": {
        "id": "fNz2N_MI5eC-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter optimization"
      ],
      "metadata": {
        "id": "GVJxsek76T2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform hyperparameter optimization**"
      ],
      "metadata": {
        "id": "tDN3F3yT7ZpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "tr, vr = loadData(os.path.join(DATA_DIR, \"ratings.dat\"), delimiter='::',\n",
        "                  seed=seed, transpose=True, valfrac=0.1)\n",
        "\n",
        "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "vm = np.greater(vr, 1e-12).astype('float32')\n",
        "\n",
        "n_m = tr.shape[0]  # number of movies\n",
        "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
        "\n",
        "#create logging dictionary\n",
        "logging_dict = {\"n_dim\": [], \"n_hid\": [], \"lambda_2\":[], \"lambda_s\":[], \"n_layers\": [], \"output_every\": [], \"n_epoch\":[], \"train_rmse\" : [], \"rmse\": []}\n",
        "\n",
        "#define parameter grid \n",
        "\n",
        "param_grid= {\n",
        "    \"n_dim\": [5],\n",
        "    \"n_hid\": [200,500,1000],\n",
        "    \"lambda_2\":[60.0],\n",
        "    \"lambda_s\":[0.013],\n",
        "    \"n_layers\": [2,5],\n",
        "    \"output_every\": [50,100],\n",
        "    \"n_epoch\":[5]\n",
        "}\n",
        "\n",
        "keys, values = zip(*param_grid.items())\n",
        "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "for params_dict in tqdm(permutations_dicts):\n",
        "\n",
        "  #reset tensorflow graph\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # Set hyper-parameters\n",
        "  n_dim = int(params_dict[\"n_dim\"])\n",
        "  n_hid = int(params_dict[\"n_hid\"])\n",
        "  lambda_2 = float(params_dict[\"lambda_2\"])\n",
        "  lambda_s = float(params_dict[\"lambda_s\"])\n",
        "  n_layers = int(params_dict[\"n_layers\"])\n",
        "  output_every = int(params_dict[\"output_every\"])  # evaluate performance on test set; breaks l-bfgs loop\n",
        "  n_epoch = int(params_dict[\"n_epoch\"])\n",
        "  verbose_bfgs = False\n",
        "  use_gpu = True\n",
        "  if not use_gpu:\n",
        "      os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "\n",
        "  \n",
        "  # Input placeholders\n",
        "  R = tf.placeholder(\"float\", [None, n_u])\n",
        "  # Instantiate network\n",
        "  y = R\n",
        "  reg_losses = None\n",
        "  for i in range(n_layers):\n",
        "      y, reg_loss = kernel_layer(y, n_hid, n_dim, lambda_s, lambda_2, activation=tf.nn.sigmoid, name=str(i))\n",
        "      reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "  prediction, reg_loss = kernel_layer(y, n_u, n_dim, lambda_s, lambda_2, activation=tf.identity, name='out')\n",
        "  reg_losses = reg_losses + reg_loss\n",
        "\n",
        "  # Compute loss (symbolic)\n",
        "  diff = tm*(R - prediction)\n",
        "  sqE = tf.nn.l2_loss(diff)\n",
        "  loss = sqE + reg_losses\n",
        "\n",
        "  # Instantiate L-BFGS Optimizer\n",
        "  optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
        "                                                                    'disp': verbose_bfgs,\n",
        "                                                                    'maxcor': 10},\n",
        "                                                    method='L-BFGS-B')\n",
        "    \n",
        "\n",
        "  # Training and validation loop\n",
        "  init = tf.global_variables_initializer()\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      print(\"num epochs to run: \", n_epoch)\n",
        "\n",
        "      for i in range(n_epoch):\n",
        "          optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
        "          pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
        "\n",
        "          error = (vm * (np.clip(pre, 1., 5.) - vr) ** 2).sum() / vm.sum() #compute validation error\n",
        "          error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
        "\n",
        "          print('.-^-._' * 12)\n",
        "          print('epoch:', i+1, 'validation rmse:', np.sqrt(error), 'train rmse:', np.sqrt(error_train))\n",
        "          print('.-^-._' * 12)\n",
        "\n",
        "          #log_hyperparameters\n",
        "          logging_dict[\"n_dim\"].append(n_dim);\n",
        "          logging_dict[\"n_hid\"].append(n_hid);\n",
        "          logging_dict[\"lambda_2\"].append(lambda_2);\n",
        "          logging_dict[\"lambda_s\"].append(lambda_s);\n",
        "          logging_dict[\"n_layers\"].append(n_layers);\n",
        "          logging_dict[\"output_every\"].append(output_every);\n",
        "          logging_dict[\"n_epoch\"].append(i+1);\n",
        "          #log train and val rmse\n",
        "          logging_dict[\"train_rmse\"].append(np.sqrt(error_train))\n",
        "          logging_dict[\"rmse\"].append(np.sqrt(error))\n",
        "          \n",
        "          #import current log to csv\n",
        "          log_df = pd.DataFrame.from_dict(logging_dict)\n",
        "          log_df.sort_values(\"rmse\", inplace = True)\n",
        "          log_df.to_csv(os.path.join(DATA_DIR,\"log_df.csv\"),index = False)\n",
        "\n",
        "  print(\"finished training, with specific hyperparameter setting\")\n"
      ],
      "metadata": {
        "id": "QAVYLtdn7iy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_df = pd.read_csv(os.path.join(DATA_DIR, \"log_df.csv\"))\n",
        "print(f\"Table with the results of the parameter tuning:\")\n",
        "display(log_df)"
      ],
      "metadata": {
        "id": "qQygrqU-9STJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions using optimal hyper-parameters"
      ],
      "metadata": {
        "id": "cUtiZJ-yS359"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train again on full data set with optimal hyper-parameters**"
      ],
      "metadata": {
        "id": "pftiP6C5TGQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "tr, vr = loadData(os.path.join(DATA_DIR, \"ratings.dat\"), delimiter='::',\n",
        "                  seed=seed, transpose=True, valfrac = -1)\n",
        "\n",
        "tm = np.greater(tr, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "vm = np.greater(vr, 1e-12).astype('float32')\n",
        "\n",
        "n_m = tr.shape[0]  # number of movies\n",
        "n_u = tr.shape[1]  # number of users (may be switched depending on 'transpose' in loadData)\n",
        "\n",
        "#reset tensorflow graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#setup models with best performing hyperparamters (log_df was sorted by ascending validation rmse)\n",
        "log_df = pd.read_csv(os.path.join(DATA_DIR, \"log_df.csv\"))\n",
        "best_params = log_df.iloc[[0]]\n",
        "\n",
        "n_dim = best_params[\"n_dim\"].values[0]\n",
        "n_hid = best_params[\"n_hid\"].values[0]\n",
        "lambda_2 = best_params[\"lambda_2\"].values[0]\n",
        "lambda_s = best_params[\"lambda_s\"].values[0]\n",
        "n_layers = best_params[\"n_layers\"].values[0]\n",
        "output_every = best_params[\"output_every\"].values[0]  # evaluate performance on test set; breaks l-bfgs loop\n",
        "n_epoch = best_params[\"n_epoch\"].values[0]\n",
        "verbose_bfgs = False\n",
        "use_gpu = True\n",
        "if not use_gpu:\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "\n",
        "\n",
        "\n",
        "# Input placeholders\n",
        "R = tf.placeholder(\"float\", [None, n_u])\n",
        "# Instantiate network\n",
        "y = R\n",
        "reg_losses = None\n",
        "for i in range(n_layers):\n",
        "    y, reg_loss = kernel_layer(y, n_hid, n_dim, lambda_s, lambda_2, activation=tf.nn.sigmoid, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "prediction, reg_loss = kernel_layer(y, n_u, n_dim, lambda_s, lambda_2, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# Compute loss (symbolic)\n",
        "diff = tm*(R - prediction)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss = sqE + reg_losses\n",
        "\n",
        "# Instantiate L-BFGS Optimizer\n",
        "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, options={'maxiter': output_every,\n",
        "                                                                  'disp': verbose_bfgs,\n",
        "                                                                  'maxcor': 10},\n",
        "                                                  method='L-BFGS-B')\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(\"num epochs to run: \", n_epoch)\n",
        "\n",
        "    for i in tqdm(range(n_epoch)):\n",
        "        optimizer.minimize(sess, feed_dict={R: tr}) #do maxiter optimization steps\n",
        "        pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
        "        error_train = (tm * (np.clip(pre, 1., 5.) - tr) ** 2).sum() / tm.sum() #compute train error\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('epoch:', i+1, 'train rmse:', np.sqrt(error_train))\n",
        "        print('.-^-._' * 12)\n",
        "\n",
        "    saver.save(sess, os.path.join(DATA_DIR, \"checkpoints/model\"))\n",
        "    print(\"saved model checkpoint\")\n",
        "    \n",
        "print(\"finished training on whole training data\")\n",
        "\n",
        "\n",
        "\n",
        "#make raw predictions of our data\n",
        "tr, vr = loadData(os.path.join(DATA_DIR, \"to_predict.dat\"), delimiter='::',\n",
        "                  seed=seed, transpose=True, valfrac = -1, shuffle_data = False)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, os.path.join(DATA_DIR, \"checkpoints/model\"))\n",
        "    pre = sess.run(prediction, feed_dict={R: tr}) #predict ratings\n",
        "    pd.DataFrame(pre).to_csv(os.path.join(DATA_DIR, \"raw_predictions.csv\"))\n",
        "\n",
        "print(\"finished raw predictions\")"
      ],
      "metadata": {
        "id": "z0nLEkKAS9Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**convert raw predictions into correct format**"
      ],
      "metadata": {
        "id": "os9ikNwOZeej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i can throw  0'th column of raw predictions away as it is just the index\n",
        "raw_predictions = pd.read_csv(os.path.join(DATA_DIR, \"raw_predictions.csv\"))\n",
        "#convert raw predictions to correct format\n",
        "output = to_predict_pd.to_numpy()\n",
        "final_predictions = raw_predictions.to_numpy()\n",
        "for id,user in enumerate(pred_users):\n",
        "  prediction = final_predictions[pred_movies[id]][user+1]\n",
        "  output[id][1] = prediction "
      ],
      "metadata": {
        "id": "mW9b57T_bDr1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame(output, columns = ['Id', 'Prediction'])\n",
        "try:\n",
        "    os.makedirs(os.path.join(DATA_DIR,\"final_predictions\"))\n",
        "except FileExistsError:\n",
        "    # directory already exists\n",
        "    pass\n",
        "submission_df.to_csv(os.path.join(DATA_DIR,\"final_predictions/kernelNet.csv\"),index = False)"
      ],
      "metadata": {
        "id": "z9n19PUmbYoN"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}